This paper uses manually annotated LENA data to assess the validity of the LENA system's CTC measure in a longitudinal corpus of English-learning children. The findings put a very important hedge on the direct (and even indirect) use of CTC as a language input measure. The authors recommend that studies using this increasingly popular measure also conduct validity analyses, perhaps along the lines of what they demonstrate in the present paper.

I am very glad the authors have written this paper and I think it makes an important and timely contribution to the field as CTC becomes popular as a proxy for child-directed or otherwise 'high quality' input that can be derived automatically from the daylong recordings. I mostly only have small comments; ones that should be relatively easy to address.

In my opinion, the only significant issue with this paper is that it shies away from making any theoretical contributions. At a minimum, the authors should clarify how their findings affect our interpretation of the previous work using CTC as an input measure; what aspects of those claims are under scrutiny given the weakness of the CTC measure? Or also perhaps: going forward, how does this methodological limitation constrain our theory development about early language development? The leanness of the theoretical contribution of this article made me wonder if the paper might be better suited to a different journal (e.g., one geared towards methods)---some theoretical discussion is warranted for the current journal. My support revision and eventual publication is contingent on the addition on some deeper theoretical contribution.

My other comments are as follows:

- Highly detailed and up-to-date reliability data for the LENA system are available in the following forthcoming paper by Cristia et al. (here linked via OSF): https://osf.io/fhs57/. The authors may find this reference useful in their Introduction.
- I really appreciate the effort that went into manual annotation, but I wish the authors would have provided a bit more information on how they actually made the measurements. For example, it was unclear to me whether they determined turn counts using the speech segments indicated in the .its file or whether they identified speech segments from scratch. This matters hugely in characterizing where LENA goes "wrong": the authors pitch overheard vs. directed speech as the primary factor causing a difference in their data vs. CTC, but perhaps much of the error could be explained already in the diarization step. Could the authors please detail their method for identifying turns and, as relevant, update the discussion about how much they think this error is inherited from diarization error or simply to do with counting overheard speech as contingent on child-speech?
- Relatedly, was 5-seconds the maximum time window used for what counts as contingent vocalization (i.e., the same limit as LENA)? Knowing how the relevant time window was delimited is be critical for replication of this/similar workflows, as called for in the discussion. Along these same lines I wondered how simultaneous speech was handled and how contingency was determined in some of the very young samples, when it may be ambiguous what constitutes a 'response' by the infant.
- The authors note that the sampling technique here (i.e., using AWC) may _overestimate_ the average LENA error, and I agree that this is an important caveat to consider (see also some discussion about sampling bias for validation in the Cristia et al. study I linked from OSF above). Going forward, it's unclear whether people who want to validate their studies should similarly choose high-volubility segments. Why not random? Or perhaps those that have high AWC but little/lots of overlap (which might indicate plenty of speech but from fewer/more speakers)? I'm sure the authors have some valuable insight here on what would be scientifically useful going forward, ideally giving this insight with respect to theory development that could come out of these measures (as mentioned above).
- On a related note, I wonder if the measures are better in the cases where the recorded children were the only child in their household (the first born, and without other children present). Could the authors check this? A similar check would be very welcome for household size if that information is available---it relates to the hypothesis about why CTC might be overestimated.
- "some of the disagreements likely stem from LENA’s random error (i.e. independent of the true number of CTs, LENA’s CTC is an estimate that uses only acoustical cues, and will therefore always make mistakes)." I don't follow. My understanding is that CTC is a derived measure from the segments in the .its file and does not therefore involve any further acoustic analysis. I thought maybe the authors were referring to diarization error, but that comes as a separate second point following this one. The only other possibility that came to mind is that CTC estimates are, to my knowledge, only computed for some types of vocal activity block (not all of them!). If that's not what the authors are referring to, they should add this as another point of potential difference between the automated and manual measure in addition to clarifying this first point about acoustic cues.
- Conducting adequate validation work would be helped *immensely* if the authors shared their detailed manuals and methods for doing so. The present methods could be provided as a sort of manual or toolbox that other researchers can build on. Could the materials be linked with the paper?

Generally speaking, I am looking forward to the publication of these findings. They stand to make a timely and important methodological point---hopefully with some theoretical/broader scientific consequences to be made clearer in the next iteration of this work.
